{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accepted-analyst",
   "metadata": {},
   "source": [
    "# Introduction to SparkML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-alabama",
   "metadata": {},
   "source": [
    "#### Tabnine Extension for intellisense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "atlantic-center",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter_tabnine/main...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "Enabling: jupyter_tabnine\n",
      "- Writing config: /home/ateeb/.jupyter\n",
      "    - Validating...\n",
      "      jupyter_tabnine  \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install jupyter-tabnine\n",
    "# !jupyter nbextension install --py jupyter_tabnine\n",
    "!jupyter nbextension enable --py jupyter_tabnine\n",
    "!jupyter serverextension enable --py jupyter_tabnine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-czech",
   "metadata": {},
   "source": [
    "### Spark Initiallization Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "floral-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# getting the directory where Spark was installed\n",
    "if 'SPARK_HOME' not in os.environ:\n",
    "    os.environ['SPARK_HOME'] = '/opt/spark'\n",
    "\n",
    "# python variable to store the root path for later reference\n",
    "SPARK_HOME = os.environ['SPARK_HOME']\n",
    "\n",
    "# adding pyspark and py4j packages paths to python path env variable\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME, \"python\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME, \"python\", \"lib\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME, \"python\", \"lib\", 'py4j-0.10.9-src.zip'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', 'pyspark.zip'))\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set('spark.executer.memory','1g')\n",
    "conf.set('spark.cores.max','2')\n",
    "\n",
    "# give name to your spark application\n",
    "conf.setAppName(\"SparkMLApp\")\n",
    "\n",
    "# create a spark context object \n",
    "# note: Execute only once otherwise results in Context Errors\n",
    "# create the spark context with 2 threads for streaming\n",
    "sc = SparkContext('local',conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-gender",
   "metadata": {},
   "source": [
    "#### Once the above script is executed you can view the Spark instance info here http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-building",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "Problem Statement\n",
    "*****************\n",
    "The input data set contains data about details of various car \n",
    "models. Based on the information provided, the goal is to come up \n",
    "with a model to predict Miles-per-gallon of a given model.\n",
    "\n",
    "Techniques Used:\n",
    "\n",
    "1. Linear Regression ( multi-variate)\n",
    "2. Data Imputation - replacing non-numeric data with numeric ones\n",
    "3. Variable Reduction - picking up only relevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-christianity",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "random-nightlife",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MPG,CYLINDERS,DISPLACEMENT,HORSEPOWER,WEIGHT,ACCELERATION,MODELYEAR,NAME',\n",
       " '18,8,307,130,3504,12,70,chevrolet chevelle malibu',\n",
       " '15,8,350,165,3693,11.5,70,buick skylark 320']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading csv file into an RDD\n",
    "autoData = sc.textFile(\"data/auto-miles-per-gallon.csv\")\n",
    "autoData.cache()\n",
    "autoData.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "decent-double",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove the first line (contains headers)\n",
    "dataLines = autoData.filter(lambda line: \"MPG\" not in line)\n",
    "dataLines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-fossil",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "productive-protein",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([18.0, 8.0, 130.0, 12.0, 70.0]),\n",
       " DenseVector([15.0, 8.0, 165.0, 11.5, 70.0]),\n",
       " DenseVector([18.0, 8.0, 150.0, 11.0, 70.0])]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# set a random value for avg hp to replace with missing values in data\n",
    "avgHP = sc.broadcast(80.0)\n",
    "\n",
    "# function to perform data cleaning\n",
    "def CleanupData(inputStr):\n",
    "    global avgHP\n",
    "    attList = inputStr.split(\",\")\n",
    "    \n",
    "    #Replace ? values with a normal value\n",
    "    hpValue = attList[3]\n",
    "    if hpValue == \"?\":\n",
    "        hpValue = avgHP.value\n",
    "        \n",
    "    #Create a row with cleaned up and converted data    \n",
    "    values = Vectors.dense([float(attList[0]), float(attList[1]), hpValue, float(attList[5]), float(attList[6])])\n",
    "    return values\n",
    "\n",
    "#Run map for cleanup\n",
    "autoVectors = dataLines.map(CleanupData)\n",
    "autoVectors.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.stat import mean\n",
    "# from pyspark.ml.stat import Summarizer\n",
    "# autoStats = Statistics.colStats(autoVectors)\n",
    "# print(\"Mean:\\n\", autoStats.mean())\n",
    "# print(\"Variance:\\n\", autoStats.variance())\n",
    "# print(\"Max:\\n\", autoStats.max())\n",
    "# print(\"Min:\\n\", autoStats.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "thrown-better",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+\n",
      "|label|        features|\n",
      "+-----+----------------+\n",
      "| 18.0|[8.0,130.0,70.0]|\n",
      "| 15.0|[8.0,165.0,70.0]|\n",
      "| 18.0|[8.0,150.0,70.0]|\n",
      "| 16.0|[8.0,150.0,70.0]|\n",
      "| 17.0|[8.0,140.0,70.0]|\n",
      "+-----+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext  = SQLContext(sc)\n",
    "\n",
    "# creating a label point\n",
    "def transformToLabelPoint(inputStr):\n",
    "    lp = (float(inputStr[0]),Vectors.dense([float(inputStr[1]), float(inputStr[2]), float(inputStr[4])]))\n",
    "    return lp\n",
    "\n",
    "autoLp = autoVectors.map(transformToLabelPoint)\n",
    "autoDf = sqlContext.createDataFrame(autoLp,[\"label\",\"features\"])\n",
    "autoDf.select(\"label\",\"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "organic-video",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t-0.775396\n",
      "1\t-0.774631\n",
      "2\t0.579267\n"
     ]
    }
   ],
   "source": [
    "# finding correlations\n",
    "numFeatures = autoDf.take(1)[0].features.size\n",
    "labelRDD  = autoDf.rdd.map(lambda lp: float(lp.label))\n",
    "\n",
    "for i in range(numFeatures):\n",
    "    featuresRDD = autoDf.rdd.map(lambda lp: lp.features[i])\n",
    "    corr = Statistics.corr(labelRDD,featuresRDD,'pearson')\n",
    "    print(\"%d\\t%g\" % (i, corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "increased-closing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351 47\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = autoDf.randomSplit([0.9,0.1])\n",
    "print( trainingData.count() ,testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "amber-remainder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept:  -10.591761761379919\n",
      "Coefficients:  [-1.9753840213069946,-0.05686563255267235,0.6704506882901058]\n"
     ]
    }
   ],
   "source": [
    "# build a Linear Regression Model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(maxIter=10)\n",
    "lrModel = lr.fit(trainingData)\n",
    "\n",
    "print(\"Intercept: \", str(lrModel.intercept))\n",
    "print(\"Coefficients: \", str(lrModel.coefficients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fossil-austin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+----------------+\n",
      "|        prediction|label|        features|\n",
      "+------------------+-----+----------------+\n",
      "|12.312252453860824| 11.0|[8.0,180.0,73.0]|\n",
      "|14.485083393204333| 13.0|[8.0,130.0,72.0]|\n",
      "| 16.02957349531131| 13.0|[8.0,150.0,76.0]|\n",
      "|14.757474653625737| 14.0|[8.0,137.0,73.0]|\n",
      "|15.472854072126546| 14.0|[8.0,148.0,75.0]|\n",
      "|14.018221430440997| 14.0|[8.0,150.0,73.0]|\n",
      "|11.824335565570697| 14.0|[8.0,165.0,71.0]|\n",
      "|11.153884877280587| 15.0|[8.0,165.0,70.0]|\n",
      "|15.359122807021203| 16.0|[8.0,150.0,75.0]|\n",
      "|19.178238211235076| 17.0|[8.0,130.0,79.0]|\n",
      "|22.925417528322292| 17.5|[6.0,110.0,77.0]|\n",
      "| 17.93913119741825| 17.5|[8.0,140.0,78.0]|\n",
      "|19.471369724108392| 18.0|[6.0,100.0,71.0]|\n",
      "|21.198393626215342| 18.0|[6.0,105.0,74.0]|\n",
      "|18.902713398581668| 18.0|[6.0,110.0,71.0]|\n",
      "| 17.99599682997092| 18.1|[8.0,139.0,78.0]|\n",
      "|23.607805118954367| 18.5| [6.0,98.0,77.0]|\n",
      "|27.456778798804976| 19.0| [4.0,88.0,76.0]|\n",
      "|22.721828802795528| 19.0| [6.0,90.0,75.0]|\n",
      "|19.471369724108392| 19.0|[6.0,100.0,71.0]|\n",
      "+------------------+-----+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predicting on the test data\n",
    "pred = lrModel.transform(testData)\n",
    "pred.select(\"prediction\",\"label\",\"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-proportion",
   "metadata": {},
   "source": [
    "### Evaluating the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "complimentary-hobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 Accuracy:  61.25065305605055\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol='prediction',labelCol='label',metricName='r2')\n",
    "print(\"r2 Accuracy: \", evaluator.evaluate(pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-glenn",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
