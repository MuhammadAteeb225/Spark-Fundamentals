{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "romantic-framework",
   "metadata": {},
   "source": [
    "# Introduction to Spark Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-narrative",
   "metadata": {},
   "source": [
    "#### Tabnine Extension for intellisense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fancy-remainder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter_tabnine/main...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n",
      "Enabling: jupyter_tabnine\n",
      "- Writing config: /home/ateeb/.jupyter\n",
      "    - Validating...\n",
      "      jupyter_tabnine  \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install jupyter-tabnine\n",
    "# !jupyter nbextension install --py jupyter_tabnine\n",
    "!jupyter nbextension enable --py jupyter_tabnine\n",
    "!jupyter serverextension enable --py jupyter_tabnine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-growth",
   "metadata": {},
   "source": [
    "### Spark Initiallization Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bulgarian-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# getting the directory where Spark was installed\n",
    "if 'SPARK_HOME' not in os.environ:\n",
    "    os.environ['SPARK_HOME'] = '/opt/spark'\n",
    "\n",
    "# python variable to store the root path for later reference\n",
    "SPARK_HOME = os.environ['SPARK_HOME']\n",
    "\n",
    "# adding pyspark and py4j packages paths to python path env variable\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME, \"python\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME, \"python\", \"lib\"))\n",
    "sys.path.insert(0,os.path.join(SPARK_HOME, \"python\", \"lib\", 'py4j-0.10.9-src.zip'))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, 'python', 'lib', 'pyspark.zip'))\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set('spark.executer.memory','1g')\n",
    "conf.set('spark.cores.max','2')\n",
    "\n",
    "# give name to your spark application\n",
    "conf.setAppName(\"SparkDTreeApp\")\n",
    "\n",
    "# create a spark context object \n",
    "# note: Execute only once otherwise results in Context Errors\n",
    "# create the spark context with 2 threads for streaming\n",
    "sc = SparkContext('local',conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-candidate",
   "metadata": {},
   "source": [
    "#### Once the above script is executed you can view the Spark instance info here http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-footwear",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "*****************\n",
    "The input data is the iris dataset. It contains recordings of \n",
    "information about flower samples. For each sample, the petal and \n",
    "sepal length and width are recorded along with the type of the \n",
    "flower. We need to use this dataset to build a decision tree \n",
    "model that can predict the type of flower based on the petal \n",
    "and sepal information.\n",
    "\n",
    "#### Techniques Used\n",
    "\n",
    "1. Decision Trees "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-coalition",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "decent-thumbnail",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sepal.Length,Sepal.Width,Petal.Length,Petal.Width,Species',\n",
       " '5.1,3.5,1.4,0.2,setosa',\n",
       " '4.9,3,1.4,0.2,setosa']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading csv file into an RDD\n",
    "irisData = sc.textFile(\"data/iris.csv\")\n",
    "irisData.persist()\n",
    "irisData.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "electric-server",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove the first line (contains headers)\n",
    "dataLines = irisData.filter(lambda line: \"Sepal\" not in line)\n",
    "dataLines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-pierre",
   "metadata": {},
   "source": [
    "### Transforming Data to labelpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "filled-romance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "| label|     features|\n",
      "+------+-------------+\n",
      "|setosa|[5.1,1.4,0.2]|\n",
      "|setosa|[4.9,1.4,0.2]|\n",
      "|setosa|[4.7,1.3,0.2]|\n",
      "|setosa|[4.6,1.5,0.2]|\n",
      "|setosa|[5.0,1.4,0.2]|\n",
      "|setosa|[5.4,1.7,0.4]|\n",
      "|setosa|[4.6,1.4,0.3]|\n",
      "|setosa|[5.0,1.5,0.2]|\n",
      "|setosa|[4.4,1.4,0.2]|\n",
      "|setosa|[4.9,1.5,0.1]|\n",
      "+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# creating sqlContext to work with dataframes\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "def transformToLabeledPoint(inputStr):\n",
    "    attList = inputStr.split(\",\")\n",
    "    lp  = (attList[4], Vectors.dense([attList[0],attList[2],attList[3]]))\n",
    "    return lp\n",
    "# from analysis it was visible the the Sepal.Width has week Correlation\n",
    "# with the target so we will be dropping this column from the dataset\n",
    "\n",
    "irisLp = dataLines.map(transformToLabeledPoint)\n",
    "irisDF = sqlContext.createDataFrame(irisLp,[\"label\",\"features\"])\n",
    "irisDF.select(\"label\",\"features\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-blocking",
   "metadata": {},
   "source": [
    "### Indexing labels to integers (DTree Requirement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "warming-amplifier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label='setosa', features=DenseVector([5.1, 1.4, 0.2]), index=0.0),\n",
       " Row(label='setosa', features=DenseVector([4.9, 1.4, 0.2]), index=0.0),\n",
       " Row(label='setosa', features=DenseVector([4.7, 1.3, 0.2]), index=0.0),\n",
       " Row(label='setosa', features=DenseVector([4.6, 1.5, 0.2]), index=0.0),\n",
       " Row(label='setosa', features=DenseVector([5.0, 1.4, 0.2]), index=0.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing nedded as per-req for Decision trees\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "# create a stringindexer object\n",
    "stringIndexer = StringIndexer(inputCol=\"label\",outputCol=\"index\")\n",
    "\n",
    "# fit the indexer model to the dataset and learn the mapping\n",
    "si_model = stringIndexer.fit(irisDF)\n",
    "\n",
    "# transform the dataset, adding a index columns\n",
    "td = si_model.transform(irisDF)\n",
    "\n",
    "td.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-intro",
   "metadata": {},
   "source": [
    "### Correlation of predictors with target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "approximate-escape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0.782561\n",
      "1\t0.949035\n",
      "2\t0.956547\n"
     ]
    }
   ],
   "source": [
    "# finding correlations\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "td_corr  = td.select(\"features\",\"index\")\n",
    "numFeatures = td_corr.take(1)[0].features.size\n",
    "labelRDD  = td_corr.rdd.values()\n",
    "\n",
    "for i in range(numFeatures):\n",
    "    featuresRDD = td_corr.rdd.map(lambda lp: lp.features[i])\n",
    "    corr = Statistics.corr(labelRDD,featuresRDD,'pearson')\n",
    "    print(\"%d\\t%g\" % (i, corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-sally",
   "metadata": {},
   "source": [
    "### Splitting Data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "compliant-franchise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 10\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = td.randomSplit([0.9,0.1])\n",
    "print( trainingData.count(), testData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-highland",
   "metadata": {},
   "source": [
    "### Creating the Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "level-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# create the model\n",
    "dtClassifier = DecisionTreeClassifier(maxDepth=2,labelCol='index')\n",
    "dtModel = dtClassifier.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "enormous-hierarchy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtModel.numNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "flexible-norwegian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtModel.depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-bahrain",
   "metadata": {},
   "source": [
    "### Making Predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "thick-company",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----------+-------------+\n",
      "|prediction|index|     label|     features|\n",
      "+----------+-----+----------+-------------+\n",
      "|       0.0|  0.0|    setosa|[5.0,1.3,0.3]|\n",
      "|       0.0|  0.0|    setosa|[5.2,1.4,0.2]|\n",
      "|       1.0|  1.0|versicolor|[5.8,4.1,1.0]|\n",
      "|       2.0|  1.0|versicolor|[5.9,4.8,1.8]|\n",
      "|       1.0|  1.0|versicolor|[6.0,4.5,1.6]|\n",
      "|       1.0|  1.0|versicolor|[6.3,4.9,1.5]|\n",
      "|       1.0|  1.0|versicolor|[6.5,4.6,1.5]|\n",
      "|       1.0|  1.0|versicolor|[6.6,4.4,1.4]|\n",
      "|       2.0|  2.0| virginica|[6.0,4.8,1.8]|\n",
      "|       2.0|  2.0| virginica|[6.4,5.3,1.9]|\n",
      "+----------+-----+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = dtModel.transform(testData)\n",
    "predictions.select(\"prediction\",\"index\",\"label\",\"features\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-dealer",
   "metadata": {},
   "source": [
    "### Evaluating the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "small-birth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"index\",predictionCol=\"prediction\",metricName=\"weightedPrecision\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-burke",
   "metadata": {},
   "source": [
    "### Making Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "seventh-darwin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|index|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|    5|\n",
      "|  2.0|       2.0|    2|\n",
      "|  1.0|       2.0|    1|\n",
      "|  0.0|       0.0|    2|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "labelList = predictions.select(\"index\",\"label\").distinct().toPandas()\n",
    "predictions.groupBy(\"index\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sixth-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "israeli-right",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       label\n",
       "0    2.0   virginica\n",
       "1    1.0  versicolor\n",
       "2    0.0      setosa"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
